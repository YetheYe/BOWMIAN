{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirements\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data files\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Processed 1 lines so far..\n",
      "Processed 101 lines so far..\n",
      "Processed 201 lines so far..\n",
      "Processed 301 lines so far..\n",
      "Processed 401 lines so far..\n",
      "Processed 501 lines so far..\n",
      "Processed 601 lines so far..\n",
      "Processed 701 lines so far..\n",
      "a first-class road movie that proves you can run away from home , but your ego and all your problems go with you .\n"
     ]
    }
   ],
   "source": [
    "def readText(src):\n",
    "    print(\"Reading lines...\")\n",
    "    lines = []\n",
    "    \n",
    "    # Read the file and split into lines\n",
    "    with open(src, 'r') as f:\n",
    "        header = f.readline()\n",
    "        cols = [c.strip() for c in header.split('\\t')]\n",
    "    \n",
    "        for li, line in enumerate(f):\n",
    "            cols = [c.strip() for c in line.split('\\t')]\n",
    "            lines.append(cols[1].lower())\n",
    "            \n",
    "            if np.mod(li, 100) == 0:\n",
    "                print('Processed {} lines so far..'.format(li+1))\n",
    "    \n",
    "    return lines\n",
    "    \n",
    "lines = readText(\"toy_sentences.tsv\")\n",
    "print(random.choice(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bongEmbed(nn.Module):\n",
    "    def __init__(self, options):\n",
    "        super(bongEmbed,self).__init__()\n",
    "\n",
    "        self.options = options\n",
    "\n",
    "        self.emb = nn.Embedding(options['n_words']+1, options['n_hid'])\n",
    "\n",
    "        self.hids = []\n",
    "        for li in range(options['n_layers']):\n",
    "            self.hids.append([\n",
    "                nn.Linear(options['n_hid'], options['n_hid']),\n",
    "                eval('nn.{}'.format(options['act']))()\n",
    "                ])\n",
    "            indim = options['n_hid']\n",
    "        for i in range(options['n_layers']):\n",
    "            print(self.hids[i][0])\n",
    "        self.hid_modules = nn.ModuleList([h[0] for h in self.hids])\n",
    "\n",
    "        self.classifier = nn.Linear(options['n_hid'], 2)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, s1, s1m):\n",
    "        print(\"################## forward ###################\")\n",
    "        s1emb = self.emb(s1)\n",
    "        print(\"s1\", s1)\n",
    "        print(\"s1m\", s1m)\n",
    "        print(\"s1emb\", s1emb)\n",
    "        s1emb = torch.mul(s1emb, s1m.unsqueeze(2).expand_as(s1emb))\n",
    "        print(\"s1emb\", s1emb)\n",
    "        s1emb = torch.sum(s1emb,1).squeeze()\n",
    "        print(\"s1emb\", s1emb)\n",
    "\n",
    "        h = s1emb\n",
    "\n",
    "        for li in range(self.options['n_layers']):\n",
    "            h = self.hids[li][0](h)\n",
    "            h = self.hids[li][1](h)\n",
    "\n",
    "        z = self.classifier(h)\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def variableFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    result = Variable(torch.LongTensor(indexes).view(-1, 1))\n",
    "    if use_cuda:\n",
    "        return result.cuda()\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "\n",
    "def variablesFromPair(pair):\n",
    "    input_variable = variableFromSentence(input_lang, pair[0])\n",
    "    target_variable = variableFromSentence(output_lang, pair[1])\n",
    "    return (input_variable, target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MAX_LENGTH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-cc47718efadf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MAX_LENGTH' is not defined"
     ]
    }
   ],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_variable, target_variable, decoder, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "\n",
    "    \n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for i in range(input_length):\n",
    "        \n",
    "        encoder_output = bagOfNgram(input_variable[i]) ##\n",
    "\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    decoder_hidden = decoder.initHidden()\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "            decoder_input = target_variable[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            ni = topi[0][0]\n",
    "\n",
    "            decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "            if ni == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [variablesFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_variable = training_pair[0]\n",
    "        target_variable = training_pair[1]\n",
    "\n",
    "        loss = train(input_variable, target_variable,\n",
    "                     decoder, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "options = OrderedDict(\n",
    "            {'n_words': 100000,\n",
    "             'order': 3,\n",
    "             'saveto': \"result.txt\",\n",
    "             'vocab': \"dictionary.pickle\",\n",
    "             'n_layers': 2,\n",
    "             'n_hid': 100,\n",
    "             'emb': 64,\n",
    "             'act': 'Tanh',\n",
    "             'batch': 64,\n",
    "             'disp_freq': 10,\n",
    "             'save_freq': 1000,\n",
    "             'val_freq': 1000,\n",
    "             'n_epochs': 100,\n",
    "             'seed': 123,\n",
    "            })\n",
    "\n",
    "emb = nn.Embedding(options[\"n_words\"], options[\"n_hid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_iterator:\n",
    "\n",
    "    def __init__(self, fname, options):\n",
    "        self.fname = fname\n",
    "        self.options = options\n",
    "\n",
    "        self.source = open(fname, 'r')\n",
    "        self.source.readline() # dump the header\n",
    "\n",
    "        self.vocab = pkl.load(open(options['vocab'], 'rb'))\n",
    "\n",
    "        self.end_of_data = False\n",
    "\n",
    "        self.nlp = spacy.load('en')\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        self.source.seek(0)\n",
    "        self.source.readline() # dump the header\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.end_of_data:\n",
    "            self.end_of_data = False\n",
    "            self.reset()\n",
    "            raise StopIteration\n",
    "\n",
    "        s1 = []\n",
    "        labels = []\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                line = self.source.readline()\n",
    "                if line == '':\n",
    "                    raise IOError\n",
    "\n",
    "                cols = [c.strip() for c in line.split('\\t')]\n",
    "                l_ = cols[0].lower()\n",
    "                if l_ == '-':\n",
    "                    continue\n",
    "\n",
    "                if len(cols) < 3:\n",
    "                    labels.append(0)\n",
    "                else:\n",
    "                    if cols[2] not in label_map:\n",
    "                        continue\n",
    "                    labels.append(label_map[cols[2]])\n",
    "                s1_ = self.process(cols[1].lower())\n",
    "                s1.append(s1_)\n",
    "\n",
    "                if len(s1) > self.options['batch']:\n",
    "                    break\n",
    "        except IOError:\n",
    "            self.end_of_data = True\n",
    "\n",
    "        if len(s1) <= 0:\n",
    "            self.end_of_data = False\n",
    "            self.reset()\n",
    "            raise StopIteration\n",
    "\n",
    "        s1, s1m = self.equalizer(s1)\n",
    "\n",
    "        return s1, s1m, labels\n",
    "\n",
    "    def equalizer(self, sents):\n",
    "        max_len = numpy.max([len(s) for s in sents])\n",
    "        sents_ = []\n",
    "        masks_ = []\n",
    "        i = 0 \n",
    "        for sent in sents:\n",
    "            s_ = [0] * max_len\n",
    "            m_ = [1] * len(sent) + [0] * (max_len - len(sent))\n",
    "            masks_.append(m_)\n",
    "            s_[:len(sent)] = sent\n",
    "            sents_.append(s_)\n",
    "        return sents_, masks_\n",
    "\n",
    "    def process(self, sent):\n",
    "        sent = sent.replace('\"', '')\n",
    "        #print(sent)\n",
    "        uwords = [t.text for t in self.nlp(str(sent))]\n",
    "        bong = []\n",
    "        for ng in set([' '.join(t).strip() for t in \n",
    "                zip(*[uwords[i:] for i in range(3)])]):\n",
    "            #print(ng)\n",
    "            if ng in self.vocab:\n",
    "                idx = self.vocab[ng]\n",
    "                if idx > self.options['n_words']:\n",
    "                    pass\n",
    "                else:\n",
    "                    bong.append(idx)\n",
    "            else:\n",
    "                pass\n",
    "        print(bong)\n",
    "        return bong\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-290be31d4628>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.tsv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-3044aa3979e6>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fname, options)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# dump the header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.tsv'"
     ]
    }
   ],
   "source": [
    "train = data_iterator('train.tsv', options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
